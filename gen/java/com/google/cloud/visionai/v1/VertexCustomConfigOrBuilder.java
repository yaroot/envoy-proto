// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: google/cloud/visionai/v1/platform.proto

package com.google.cloud.visionai.v1;

public interface VertexCustomConfigOrBuilder extends
    // @@protoc_insertion_point(interface_extends:google.cloud.visionai.v1.VertexCustomConfig)
    com.google.protobuf.MessageOrBuilder {

  /**
   * <pre>
   * The max prediction frame per second. This attribute sets how fast the
   * operator sends prediction requests to Vertex AI endpoint. Default value is
   * 0, which means there is no max prediction fps limit. The operator sends
   * prediction requests at input fps.
   * </pre>
   *
   * <code>int32 max_prediction_fps = 1;</code>
   * @return The maxPredictionFps.
   */
  int getMaxPredictionFps();

  /**
   * <pre>
   * A description of resources that are dedicated to the DeployedModel, and
   * that need a higher degree of manual configuration.
   * </pre>
   *
   * <code>.google.cloud.visionai.v1.DedicatedResources dedicated_resources = 2;</code>
   * @return Whether the dedicatedResources field is set.
   */
  boolean hasDedicatedResources();
  /**
   * <pre>
   * A description of resources that are dedicated to the DeployedModel, and
   * that need a higher degree of manual configuration.
   * </pre>
   *
   * <code>.google.cloud.visionai.v1.DedicatedResources dedicated_resources = 2;</code>
   * @return The dedicatedResources.
   */
  com.google.cloud.visionai.v1.DedicatedResources getDedicatedResources();
  /**
   * <pre>
   * A description of resources that are dedicated to the DeployedModel, and
   * that need a higher degree of manual configuration.
   * </pre>
   *
   * <code>.google.cloud.visionai.v1.DedicatedResources dedicated_resources = 2;</code>
   */
  com.google.cloud.visionai.v1.DedicatedResourcesOrBuilder getDedicatedResourcesOrBuilder();

  /**
   * <pre>
   * If not empty, the prediction result will be sent to the specified cloud
   * function for post processing.
   * * The cloud function will receive AppPlatformCloudFunctionRequest where
   * the annotations field will be the json format of proto PredictResponse.
   * * The cloud function should return AppPlatformCloudFunctionResponse with
   * PredictResponse stored in the annotations field.
   * * To drop the prediction output, simply clear the payload field in the
   * returned AppPlatformCloudFunctionResponse.
   * </pre>
   *
   * <code>string post_processing_cloud_function = 3;</code>
   * @return The postProcessingCloudFunction.
   */
  java.lang.String getPostProcessingCloudFunction();
  /**
   * <pre>
   * If not empty, the prediction result will be sent to the specified cloud
   * function for post processing.
   * * The cloud function will receive AppPlatformCloudFunctionRequest where
   * the annotations field will be the json format of proto PredictResponse.
   * * The cloud function should return AppPlatformCloudFunctionResponse with
   * PredictResponse stored in the annotations field.
   * * To drop the prediction output, simply clear the payload field in the
   * returned AppPlatformCloudFunctionResponse.
   * </pre>
   *
   * <code>string post_processing_cloud_function = 3;</code>
   * @return The bytes for postProcessingCloudFunction.
   */
  com.google.protobuf.ByteString
      getPostProcessingCloudFunctionBytes();

  /**
   * <pre>
   * If true, the prediction request received by custom model will also contain
   * metadata with the following schema:
   * 'appPlatformMetadata': {
   *       'ingestionTime': DOUBLE; (UNIX timestamp)
   *       'application': STRING;
   *       'instanceId': STRING;
   *       'node': STRING;
   *       'processor': STRING;
   *  }
   * </pre>
   *
   * <code>bool attach_application_metadata = 4;</code>
   * @return The attachApplicationMetadata.
   */
  boolean getAttachApplicationMetadata();
}
